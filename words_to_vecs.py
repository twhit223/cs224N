import numpy as np
import nltk
import random
import pickle
import code
import string
import os
import re
import copy
import gensim
import tensorflow as tf

from tensorflow.contrib import lookup
from tensorflow.python.platform import gfile

from keras.models import load_model

from collections import Counter

from nltk.tokenize import sent_tokenize, word_tokenize
from nltk.corpus import stopwords

from sklearn.preprocessing import OneHotEncoder

# Set variables
num_features = 10000
len_revs = 100
test = False
MAX_DOCUMENT_LENGTH = 200
PADWORD = 'PAAAAAD'

### Import the reviews as a list
def import_reviews(filename):
  
  revs_raw = open("data/" + filename, "r").read().split('\n')
  if test: revs_raw = revs_raw[0:1000]
  print("Preprocessing reviews...")
  processed_revs = []
  # scores = np.array((len(revs_raw), ))
  scores = []
  all_words = []
  print(len(revs_raw))
  count = 0
  max_length = 0 

  for rev in revs_raw:
    
    # Get the categories
    if rev == '':
      continue
    rev = rev.split('|~|')
    pro = rev[0]
    con = rev[1]
    # adv = rev[2]
    score = rev[3]
    # pre_post = rev[4]
    # rev_id = rev[5]
    # company_id = rev[6]
    # industry = rev[7]
    # comp_good_bad = rev[8]

    # Tokenize the words and cut each off at 100 words 
    pro_words = word_tokenize(pro.lower())[0:len_revs]
    con_words = word_tokenize(con.lower())[0:len_revs]
    rev_words = pro_words + con_words
    all_words.extend(rev_words)
    rev_line = ' '.join(rev_words)
    if len(rev_words) > max_length: max_length = len(rev_words)

    processed_revs.append(rev_line)
    scores.append(int(score))

  return all_words, processed_revs, scores


### Helper function that flattens a list of lists into a list. 
def flatten(seq,container=None):
    if container is None:
        container = []
    for s in seq:
        if hasattr(s,'__iter__'):
            flatten(s,container)
        else:
            container.append(s)
    return container

### Helper function that saves a list a a pickle file
def save_as_pickle_py2(data, filename):
  with open(filename, 'wb') as f:
    pickle.dump(data, f, protocol = 3)
  return

### Helper function that loads a list from a pickle file
def load_pickle(filename):
  with open(filename, 'rb') as f:
    data = pickle.load(f)
  return data


def one_hot_encode(all_words, revs, num_words, scores):
  """Convert the reviews to integer vectors and one-hot vectors
    Args: 
      all_words: list of all words contained in reviews; generated by import_reviews()
      revs: list of lists contained word tokenized lists of each review; generated by import_reviews()
      num_words: integer that specifies the number of words to be encoded in the one-hot vector; all other words will be encoded as 'UNK'
    Returns:
      revs_one_hot_encoded: numpy array of shape(num_revs, 2*len_revs*num_words) containing one review per row
      revs_integer_encoded: numpy array of shape(num_revs, 2*len_revs) containing one review per row
      count: a Counter object that contains the number of times each of the words appears in the text
      dictionary: dict with each of the words mapped to an integer
      reversed_dictionary: dict with each of the integers between [0, num_words] mapped to a word
  """

  # Get the 10k most common words as a dict with the form word:position
  print("Creating bag of words dictionary...")
  count = [['PAAAAAD', -1]]
  count.extend(Counter(all_words).most_common(num_words - 2))
  count.extend([['UUUNKKK', -1]])
  dictionary = dict()
  for word, _ in count:
    dictionary[word] = len(dictionary)
  reversed_dictionary = dict(zip(dictionary.values(), dictionary.keys()))
  inv_dict_list = []
  for i in range(0,len(reversed_dictionary)):
    inv_dict_list.append(reversed_dictionary[i])

  
  # Create a 200-length integer vector for each review
  print("Integer encoding reviews...")
  revs_int_encoded = [[dictionary[word] if word in dictionary else dictionary['UUUNKKK'] for word in rev] + [dictionary['PAAAAAD']]*(2*len_revs-len(rev)) for rev in revs]
  output = []
  for i in range(0,len(scores)):
    output.append((revs_int_encoded[i][0:MAX_DOCUMENT_LENGTH], scores[i]-1))

  # # Create the one-hot vectors from the integer vectors
  # print("One-hot encoding reviews...")
  # enc = OneHotEncoder(n_values = 10000)
  # enc.fit(revs_int_encoded)
  # revs_one_hot_encoded = enc.transform(revs_int_encoded).toarray()
  # print("All encoding complete...")

  # # Get the counts for 'PAD' and 'UNK' and create the lookup table
  # flat = flatten(revs_int_encoded)
  # count[0][1] = flat.count()
  # count[-1][1] = flat.count(1)

  # # Convert revs_int_encoded to numpy array
  # revs_int_encoded = np.array(revs_int_encoded)

  # return revs_one_hot_encoded, revs_int_encoded, count, dictionary, reversed_dictionary, inv_dict_list
  return output, dictionary, reversed_dictionary, inv_dict_list


### Convert the reviews to word2vec bag of words vectors
def word2vec_encode(word2int_dict, word2int_reversed_dict):
  print("Encoding word2vec embeddings...")
  model = gensim.models.KeyedVectors.load_word2vec_format('data/GoogleNews-vectors-negative300.bin', binary=True)
  vocab_size = len(word2int_reversed_dict)
  embed_size = 300
  embeddings = np.zeros((vocab_size, embed_size), dtype = np.float32)
  for k,v in word2int_dict.items():
    if k == 'PAAAAAD':
      embeddings[v] = [0.0]*embed_size
    elif k.strip() in model:
      embeddings[v] = model[k]
    else:
      # Encode the unkown vectors
      embeddings[v] = [0.00000001]*embed_size

  # zero = np.zeros(300)
  # revs_word2vec_encoded = [[model[word] if word in model.wv.vocab else zero for word in rev] for rev in revs]

  return embeddings


### Convert the reviews to GloVe vectors
def load_glove_model(gloveFile):
    print ("Loading Glove Model")
    f = open(gloveFile,'r')
    model = {}
    for line in f:
        splitLine = line.split()
        word = splitLine[0]
        embedding = np.array([float(val) for val in splitLine[1:]])
        model[word] = embedding
    print ("Done.",len(model)," words loaded!")
    return model

def glove_encode(word2int_dict, word2int_reversed_dict):

  model = load_glove_model('data/glove.42B.300d.txt')
  vocab_size = len(word2int_reversed_dict)
  embed_size = 300
  embeddings = np.zeros((vocab_size, embed_size), dtype = np.float32)
  for k,v in word2int_dict.items():
    if k == 'PAAAAAD':
      embeddings[v] = [0.0]*embed_size
    elif k.strip() in model:
      embeddings[v] = model[k]
    else:
      # Encode the unkown vectors
      embeddings[v] = [0.00000001]*embed_size

  # zero = np.zeros(100)
  # revs_glove_encoded = [[model[word] if word in model else zero for word in rev] for rev in revs]

  return embeddings

### Convert the words to CoVe vectors. Cannot do this in tensorflow because cove loads from keras and requires the whole sentence
def cove_encode(revs_int_encoded, glove_embedding):

  print("Encoding reviews with CoVe...")
  # Get the reviews and scores for each area of the input
  revs = [rev[0] for rev in revs_int_encoded]
  scores = [rev[1] for rev in revs_int_encoded]
  revs_glove_encoded = [[glove_embedding[num] for num in rev] for rev in revs]

  # Convert to a numpy array to pass into the glove encoder
  revs_glove_encoded = np.array(revs_glove_encoded)

  # Load the model and convert the reviews
  print("Loading CoVe model...")
  model = load_model('Keras_CoVe.h5')
  print("Encoding reviews...")
  revs_cove_encoded = model.predict(revs_glove_encoded)
  print("Appending scores...")
  code.interact(local = locals())

  # for rev in revs_glove_encoded:
  #   rev_reshaped = np.reshape(rev, (1, len(rev), 300))
  #   revs_cove_encoded.append(cove_model.predict(rev))

  return revs_cove_encoded

# # Convert the reviews to be integer encoded
# filename = 'reviews_all_public.txt'
# all_words, revs, scores = import_reviews(filename)
# code.interact(local = locals())
# output, word2int_dict, word2int_reversed_dict, inv_dict_list = one_hot_encode(all_words, revs, num_features, scores)
# helper = (word2int_dict, MAX_DOCUMENT_LENGTH)
# save_as_pickle_py2(helper, 'data/test_helper.pickle')
# save_as_pickle_py2(word2int_reversed_dict, 'data/test_word2int_reversed_dict.pickle')

# # Split the output into train + dev, and test sets
# print('Saving reviews...')
# cutoff_dev = int(len(output)* 0.8)
# cutoff_test = int(len(output) * 0.9)
# train_set = output[0:cutoff_dev]
# dev_set = output[cutoff_dev:cutoff_test]
# test_set = output[cutoff_test:]
# save_as_pickle_py2(output, 'data/public_companies_train_data.pickle')
# save_as_pickle_py2(output, 'data/public_companies_dev_data.pickle')
# save_as_pickle_py2(output, 'data/public_companies_test_data.pickle')
# print('Saved all reviews...')
# code.interact(local = locals())

# # Get the word2vec embedding of the integer reviews
# word2vec_embedding = word2vec_encode(word2int_dict, word2int_reversed_dict)
# save_as_pickle_py2(word2vec_embedding, 'data/public_companies_word2vec_embeddings.pickle')

# # Get the GloVe embedding of the integer reviews
# glove_embedding = glove_encode(word2int_dict, word2int_reversed_dict)
# save_as_pickle_py2(glove_embedding, 'data/public_companies_glove_embeddings.pickle')

# # Get the CoVe embedding reviews  -- TAKES TOO LONG. CANNOT RUN
# glove_embedding = load_pickle('data/test_glove_embeddings.pickle')
# output = load_pickle('data/test_data.pickle')
# revs_cove_encoded = cove_encode(output, glove_embedding)
# save_as_pickle_py2(revs_cove_encoded, 'data/test_cove_encoded_revs.pickle')

code.interact(local = locals())
