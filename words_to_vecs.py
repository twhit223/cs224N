import numpy as np
import nltk
import random
import pickle
import code
import string
import os
import re
import copy
import gensim
import tensorflow as tf

from tensorflow.contrib import lookup
from tensorflow.python.platform import gfile

from keras.models import load_model

from collections import Counter

from nltk.tokenize import sent_tokenize, word_tokenize
from nltk.corpus import stopwords

from sklearn.preprocessing import OneHotEncoder

# Set variables
num_features = 10000
len_revs = 100
test = True

### Import the reviews as a list
def import_reviews(filename):
  
  revs_raw = open("data/" + filename, "r").read().split('\n')
  if test: revs_raw = revs_raw[0:1000]
  print("Preprocessing reviews...")
  processed_revs = []
  scores = np.array((len(revs_raw), ))
  all_words = []
  print(len(revs_raw))
  count = 0
  max_length = 0 

  for rev in revs_raw:
    
    # Get the categories
    if rev == '':
      continue
    rev = rev.split('||')
    pro = rev[0]
    con = rev[1]
    # adv = rev[2]
    score = rev[3]
    # pre_post = rev[4]
    # rev_id = rev[5]
    # company_id = rev[6]
    # industry = rev[7]
    # comp_good_bad = rev[8]

    # Tokenize the words and cut each off at 100 words 
    pro_words = word_tokenize(pro.lower())[0:len_revs]
    con_words = word_tokenize(con.lower())[0:len_revs]
    rev_words = pro_words + con_words
    all_words.extend(rev_words)
    rev_line = ' '.join(rev_words)
    if len(rev_words) > max_length: max_length = len(rev_words)

    processed_revs.append(rev_line)
    np.append(scores, score)

  return all_words, processed_revs, scores


### Helper function that flattens a list of lists into a list. 
def flatten(seq,container=None):
    if container is None:
        container = []
    for s in seq:
        if hasattr(s,'__iter__'):
            flatten(s,container)
        else:
            container.append(s)
    return container

def one_hot_encode(all_words, revs, num_words):
  """Convert the reviews to integer vectors and one-hot vectors
    Args: 
      all_words: list of all words contained in reviews; generated by import_reviews()
      revs: list of lists contained word tokenized lists of each review; generated by import_reviews()
      num_words: integer that specifies the number of words to be encoded in the one-hot vector; all other words will be encoded as 'UNK'
    Returns:
      revs_one_hot_encoded: numpy array of shape(num_revs, 2*len_revs*num_words) containing one review per row
      revs_integer_encoded: numpy array of shape(num_revs, 2*len_revs) containing one review per row
      count: a Counter object that contains the number of times each of the words appears in the text
      dictionary: dict with each of the words mapped to an integer
      reversed_dictionary: dict with each of the integers between [0, num_words] mapped to a word
  """

  # Get the 10k most common words as a dict with the form word:position
  print("Creating bag of words dictionary...")
  count = [['PAAAAAD', -1]]
  count.extend(Counter(all_words).most_common(num_words - 2))
  count.extend([['UUUNKKK', -1]])
  dictionary = dict()
  for word, _ in count:
    dictionary[word] = len(dictionary)
  reversed_dictionary = dict(zip(dictionary.values(), dictionary.keys()))
  inv_dict_list = []
  for i in range(0,len(reversed_dictionary)):
    inv_dict_list.append(reversed_dictionary[i])


  # code.interact(local = locals())
  
  # # Create a 200-length integer vector for each review
  # print("Integer encoding reviews...")
  # revs_int_encoded = [[dictionary[word] if word in dictionary else dictionary['UUUNKKK'] for word in rev] + [dictionary['PAAAAAD']]*(2*len_revs-len(rev)) for rev in revs]
  
  # # Create the one-hot vectors from the integer vectors
  # print("One-hot encoding reviews...")
  # enc = OneHotEncoder(n_values = 10000)
  # enc.fit(revs_int_encoded)
  # revs_one_hot_encoded = enc.transform(revs_int_encoded).toarray()
  # print("All encoding complete...")

  # # Get the counts for 'PAD' and 'UNK' and create the lookup table
  # flat = flatten(revs_int_encoded)
  # count[0][1] = flat.count()
  # count[-1][1] = flat.count(1)

  # # Convert revs_int_encoded to numpy array
  # revs_int_encoded = np.array(revs_int_encoded)

  # return revs_one_hot_encoded, revs_int_encoded, count, dictionary, reversed_dictionary, inv_dict_list
  return dictionary, reversed_dictionary, inv_dict_list


### Convert the reviews to word2vec bag of words vectors
def word2vec_embed(word2int_dict, word2int_reversed_dict):

  model = gensim.models.Word2Vec.load_word2vec_format('data/GoogleNews-vectors-negative300.bin', binary=True)
  vocab_size = len(word2int_reversed_dict)
  embed_size = 300
  embeddings = np.zeroes((vocab_size, embed_size))
  for k,v in word2int_dict.items():
    if k == 'PAAAAAD':
      embeddings[v] = [0.0]*embed_size
    elif k == 'UUUNKKK':
      embeddings[v] = [0.00000001]*embed_size
    else:
      embeddings[v] = model[k]

  # zero = np.zeros(300)
  # revs_word2vec_encoded = [[model[word] if word in model.wv.vocab else zero for word in rev] for rev in revs]

  return word2vec_embedding


### Convert the reviews to GloVe vectors
def load_glove_model(gloveFile):
    print ("Loading Glove Model")
    f = open(gloveFile,'r')
    model = {}
    for line in f:
        splitLine = line.split()
        word = splitLine[0]
        embedding = np.array([float(val) for val in splitLine[1:]])
        model[word] = embedding
    print ("Done.",len(model)," words loaded!")
    return model

def glove_encode(word2int_dict, word2int_reversed_dict):

  model = load_glove_model('data/glove.42B.300d.txt')
  vocab_size = len(word2int_reversed_dict)
  embed_size = 300
  embeddings = np.zeroes((vocab_size, embed_size))
  for k,v in word2int_dict.items():
    if k == 'PAAAAAD':
      embeddings[v] = [0.0]*embed_size
    elif k == 'UUUNKKK':
      embeddings[v] = [0.00000001]*embed_size
    else:
      embeddings[v] = model[k]

  # zero = np.zeros(100)
  # revs_glove_encoded = [[model[word] if word in model else zero for word in rev] for rev in revs]

  return embeddings

# ### Convert the words to CoVe vectors
# def cove_encode(revs_glove_encoded):

#   cove_model = load_model('Keras_CoVe.h5')
#   # Pass each review into the glove model
#   revs_cove_encoded = np.array()
#   code.interact(local = locals())
#   for rev in revs_glove_encoded:
#     rev_reshaped = np.reshape(rev, (1, len(rev), 300))
#     revs_cove_encoded.append(cove_model.predict(rev))

#   return revs_cove_encoded


filename = 'reviews_control_private.txt'
all_words, revs, scores = import_reviews(filename)
word2int_dict, word2int_reversed_dict, inv_dict_list = one_hot_encode(all_words, revs, num_features)
# revs_one_hot_encoded, revs_int_encoded, count, word2int_dict, word2int_reversed_dict, inv_dict_list = one_hot_encode(all_words, revs, num_features)
# word2vec_embedding = word2vec_encode(word2int_dict, word2int_reversed_dict)
# glove_embedding = glove_encode(word2int_dict, word2int_reversed_dict)
# revs_cove_encoded = cove_encode(revs_glove_encoded)

### THINK ABOUT HOW TO DEAL WITH A REVIEW OR LIST OF REVIEWS
# Given a review: pros + cons
# 1. Truncate pros and cons at len_revs and concatenate
  # This is the given output of 'revs' from import_reviews() where each line is a single string with the pros + cons
MAX_DOCUMENT_LENGTH = 200
PADWORD = 'PAAAAAD'
trevs = revs[0:2]
print(trevs)
# 2. Map the review to integers based on the vocabulary
  # Here we should use tensorflow

# # This is just a test, but it seems to be working
# table = tf.contrib.lookup.index_table_from_tensor(
#     mapping=inv_dict_list, num_oov_buckets=1, default_value=-1)
# revs_to_map = tf.constant(trevs[0].split())
# numbers = table.lookup(revs_to_map)
# with tf.Session() as sess:
#   tf.tables_initializer().run()
#   print("{} --> {}".format(trevs[0], numbers.eval()))
#   code.interact(local = locals())

# This works. It takes the reviews, creates a tf.constant(), converts the strings to words, and pads the reviews to length of 200. 
# The output is what we should be passing into the input placeholder
def create_input(revs, inv_dict_list):

  # Create a lookup table with the inv_dict_list. The default value is -1 which should be 'UUUNKKK'
  table = tf.contrib.lookup.index_table_from_tensor(
    mapping=inv_dict_list, num_oov_buckets=1, default_value=-1)

  # Get the revs as a tf.constant() and convert them into their words
  tf_revs = tf.constant(revs)
  words = tf.string_split(tf_revs)

  # Create a dense representation of the words and convert to ints
  densewords = tf.sparse_tensor_to_dense(words, default_value = PADWORD)
  words_as_ints = table.lookup(densewords)

  # Pad the words_as_ints to MAX_DOCUMENT_LENGTH so that there is always a consistent length
  padding = tf.constant([[0,0],[0,MAX_DOCUMENT_LENGTH]])
  padded = tf.pad(words_as_ints, padding)
  padded = tf.reshape(padded, [len(revs), MAX_DOCUMENT_LENGTH])

  with tf.Session() as sess:
    tf.tables_initializer().run()
    code.interact(local = locals())

  return padded


int_input = create_input(revs, inv_dict_list)

# 3. Do whatever embedding is selected (one-hot, word2vec, glove, cove)
# 4. Do model shit... (Fabian)

code.interact(local = locals())