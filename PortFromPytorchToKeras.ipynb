{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch LSTM implementation\n",
    "#### Reference: \n",
    "- http://pytorch.org/docs/master/nn.html#torch.nn.LSTM\n",
    "\n",
    "LSTM math in PyTorch\n",
    "![pytorch_lstm_math.png](resources/pytorch_lstm_math.png)\n",
    "These variables in the above equations are represented by 4 Matrices\n",
    "![keras_lstm_math.png](resources/pytorch_matrices.png)\n",
    "\n",
    "## Keras LSTM implementation\n",
    "#### Reference\n",
    "- https://keras.io/layers/recurrent/#lstm\n",
    "- https://github.com/keras-team/keras/blob/2.1.3/keras/layers/recurrent.py\n",
    "\n",
    "LSTM math in Keras\n",
    "![keras_lstm_math.png](resources/keras_lstm_math.png)\n",
    "\n",
    "#### Note\n",
    "* Keras only have one bias for forget gate unlike pytorch which has two biases $b_{ig}$ & $b_{hg}$ \n",
    "* The kernel weights are transposed, while PyTorch they aren't\n",
    "* The default `recurrent_activation='hard_sigmoid'`in Keras, while in pytorch is `sigmoid`, hence the default in keras needs to be overridden\n",
    "* PyTorch implementation by default masks inputs with values 0.0 (though the behavior is not consitent), But we have Keras implementation to mask padding\n",
    "* At the time of porting, keras has issue with using Masking along with Bidirectional layer - https://github.com/keras-team/keras/issues/3086 ,a short-cut fix is applied, where the output of the final Bi-LSTM is removed off of prediction for padded field, refer below behaviour before the shortcut fix\n",
    "![keras_bidirectional_masking_issue.png](resources/keras_bidirectional_masking_issue.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential, Model, Input\n",
    "from keras.layers import Dense, Activation, Bidirectional\n",
    "from keras.layers import LSTM, Multiply, Lambda\n",
    "from keras.layers.core import Masking\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from cove import MTLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Loading PyTorch CoVe model\n",
    "pytorch_network = MTLSTM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Building Keras MTLSTM model without short-cut fix for keras masking + Bidirectional issue\n",
    "keras_model = Sequential()\n",
    "keras_model.add(Masking(mask_value=0.,input_shape=(None,300)))\n",
    "keras_model.add(Bidirectional(LSTM(300, return_sequences=True, recurrent_activation='sigmoid', name='lstm1'),name='bidir_1'))\n",
    "keras_model.add(Bidirectional(LSTM(300, return_sequences=True, recurrent_activation='sigmoid', name='lstm2'),name='bidir_2'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Building Keras MTLSTM model with short-cut fix for keras masking + Bidirectional issue\n",
    "x = Input(shape=(None,300))\n",
    "y = Masking(mask_value=0.,input_shape=(None,300))(x)\n",
    "y = Bidirectional(LSTM(300, return_sequences=True, recurrent_activation='sigmoid', name='lstm1'),name='bidir_1')(y)\n",
    "y = Bidirectional(LSTM(300, return_sequences=True, recurrent_activation='sigmoid', name='lstm2'),name='bidir_2')(y)\n",
    "\n",
    "# These 2 layer are short-cut fix for the issue - \n",
    "y_rev_mask_fix = Lambda(lambda x: K.cast(K.any(K.not_equal(x, 0.), axis=-1, keepdims=True), K.floatx()))(x)\n",
    "y = Multiply()([y,y_rev_mask_fix])\n",
    "\n",
    "keras_model = Model(inputs=x,outputs=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, None, 300)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "masking_2 (Masking)             (None, None, 300)    0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidir_1 (Bidirectional)         (None, None, 600)    1442400     masking_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "bidir_2 (Bidirectional)         (None, None, 600)    2162400     bidir_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, None, 1)      0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "multiply_1 (Multiply)           (None, None, 600)    0           bidir_2[0][0]                    \n",
      "                                                                 lambda_1[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 3,604,800\n",
      "Trainable params: 3,604,800\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "keras_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wt_dict = pytorch_network.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Pytorch params to be ported---\n",
      "\n",
      "rnn.weight_ih_l0:(1200, 300)\n",
      "rnn.weight_hh_l0:(1200, 300)\n",
      "rnn.bias_ih_l0:(1200,)\n",
      "rnn.bias_hh_l0:(1200,)\n",
      "rnn.weight_ih_l0_reverse:(1200, 300)\n",
      "rnn.weight_hh_l0_reverse:(1200, 300)\n",
      "rnn.bias_ih_l0_reverse:(1200,)\n",
      "rnn.bias_hh_l0_reverse:(1200,)\n",
      "rnn.weight_ih_l1:(1200, 600)\n",
      "rnn.weight_hh_l1:(1200, 300)\n",
      "rnn.bias_ih_l1:(1200,)\n",
      "rnn.bias_hh_l1:(1200,)\n",
      "rnn.weight_ih_l1_reverse:(1200, 600)\n",
      "rnn.weight_hh_l1_reverse:(1200, 300)\n",
      "rnn.bias_ih_l1_reverse:(1200,)\n",
      "rnn.bias_hh_l1_reverse:(1200,)\n"
     ]
    }
   ],
   "source": [
    "print('---Pytorch params to be ported---\\n')\n",
    "for param in wt_dict.keys():\n",
    "    print(param+':'+str(wt_dict[param].numpy().shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Keras params to be set---\n",
      "\n",
      "\"bidir_1_1/forward_lstm1/kernel:0\" :(300, 1200)\n",
      "\"bidir_1_1/forward_lstm1/recurrent_kernel:0\" :(300, 1200)\n",
      "\"bidir_1_1/forward_lstm1/bias:0\" :(1200,)\n",
      "\"bidir_1_1/backward_lstm1/kernel:0\" :(300, 1200)\n",
      "\"bidir_1_1/backward_lstm1/recurrent_kernel:0\" :(300, 1200)\n",
      "\"bidir_1_1/backward_lstm1/bias:0\" :(1200,)\n",
      "\"bidir_2_1/forward_lstm2/kernel:0\" :(600, 1200)\n",
      "\"bidir_2_1/forward_lstm2/recurrent_kernel:0\" :(300, 1200)\n",
      "\"bidir_2_1/forward_lstm2/bias:0\" :(1200,)\n",
      "\"bidir_2_1/backward_lstm2/kernel:0\" :(600, 1200)\n",
      "\"bidir_2_1/backward_lstm2/recurrent_kernel:0\" :(300, 1200)\n",
      "\"bidir_2_1/backward_lstm2/bias:0\" :(1200,)\n"
     ]
    }
   ],
   "source": [
    "print('---Keras params to be set---\\n')\n",
    "for i in range(0,len(keras_model.layers)):\n",
    "    for e in zip(keras_model.layers[i].trainable_weights, keras_model.layers[i].get_weights()):\n",
    "        print('\"%s\" :%s' % (e[0].name,e[1].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_wt(wt_dict,key,no_splits):\n",
    "    wt_splits = []\n",
    "    wt = wt_dict[key]\n",
    "    wt = wt.numpy()\n",
    "    hidden_size = wt.shape[0]/no_splits\n",
    "    for i in range(0,no_splits):\n",
    "        wt_splits.append(wt[int(hidden_size*i):int(hidden_size*(i+1))])\n",
    "    return tuple(wt_splits)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def set_keras_wt(model, node_name, new_wt):\n",
    "    for layer in range(len(model.layers)):\n",
    "        i = 0\n",
    "        layer_wts = model.layers[layer].get_weights()\n",
    "        for e in zip(model.layers[layer].trainable_weights, model.layers[layer].get_weights()):\n",
    "            if re.compile(node_name).search(e[0].name):\n",
    "            #if e[0].name == node_name:\n",
    "                print('setting weigths for:'+e[0].name)\n",
    "                layer_wts[i] = new_wt\n",
    "                model.layers[layer].set_weights(layer_wts)\n",
    "                break\n",
    "            i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def port_wts(torch_wts,torch_node_name,pytorch_layer,keras_model,keras_node_name, reverse=False):\n",
    "    torch_reverse = ''\n",
    "    if reverse:\n",
    "        torch_reverse = '_reverse'\n",
    "    W_ii,W_if,W_ig,W_io = get_wt(torch_wts,torch_node_name+'.weight_ih_l'+str(pytorch_layer)+torch_reverse,4)\n",
    "    W_hi,W_hf,W_hg,W_ho = get_wt(torch_wts,torch_node_name+'.weight_hh_l'+str(pytorch_layer)+torch_reverse,4)\n",
    "    b_ii,b_if,b_ig,b_io = get_wt(torch_wts,torch_node_name+'.bias_ih_l'+str(pytorch_layer)+torch_reverse,4)\n",
    "    b_hi,b_hf,b_hg,b_ho = get_wt(torch_wts,torch_node_name+'.bias_hh_l'+str(pytorch_layer)+torch_reverse,4)\n",
    "    \n",
    "    b_i = b_ii + b_hi\n",
    "    b_f = b_if + b_hf\n",
    "    b_g = b_ig + b_hg\n",
    "    b_o = b_io + b_ho\n",
    "    \n",
    "    kernel = []\n",
    "    kernel.extend(W_ii)\n",
    "    kernel.extend(W_if)\n",
    "    kernel.extend(W_ig)\n",
    "    kernel.extend(W_io)\n",
    "    kernel = np.array(kernel)\n",
    "    kernel = kernel.transpose()\n",
    "    #print('kernel:'+str(kernel.shape))\n",
    "    \n",
    "    recurrent_kernel = []\n",
    "    recurrent_kernel.extend(W_hi)\n",
    "    recurrent_kernel.extend(W_hf)\n",
    "    recurrent_kernel.extend(W_hg)\n",
    "    recurrent_kernel.extend(W_ho)\n",
    "    recurrent_kernel = np.array(recurrent_kernel)\n",
    "    recurrent_kernel = recurrent_kernel.transpose()\n",
    "    #print('recurrent_kernel:'+str(recurrent_kernel.shape))\n",
    "    \n",
    "    bias = []\n",
    "    bias.extend(b_i)\n",
    "    bias.extend(b_f)\n",
    "    bias.extend(b_g)\n",
    "    bias.extend(b_o)\n",
    "    bias = np.array(bias)\n",
    "    bias = bias.transpose()\n",
    "    #print('bias:'+str(bias.shape))\n",
    "    \n",
    "    keras_direction = 'forward'\n",
    "    if reverse:\n",
    "        keras_direction = 'backward'\n",
    "    keras_node_name = keras_node_name.format(keras_direction)\n",
    "    \n",
    "    set_keras_wt(keras_model,keras_node_name+'kernel:0',kernel)\n",
    "    set_keras_wt(keras_model,keras_node_name+'bias:0',bias)\n",
    "    set_keras_wt(keras_model,keras_node_name+'recurrent_kernel:0',recurrent_kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting weigths for:bidir_1_1/forward_lstm1/kernel:0\n",
      "setting weigths for:bidir_1_1/forward_lstm1/bias:0\n",
      "setting weigths for:bidir_1_1/forward_lstm1/recurrent_kernel:0\n",
      "setting weigths for:bidir_1_1/backward_lstm1/kernel:0\n",
      "setting weigths for:bidir_1_1/backward_lstm1/bias:0\n",
      "setting weigths for:bidir_1_1/backward_lstm1/recurrent_kernel:0\n"
     ]
    }
   ],
   "source": [
    "#Porting Bi-LSTM layer - 1\n",
    "port_wts(wt_dict,'rnn',0,keras_model,'bidir_1.*/{}_lstm1/',reverse=False)\n",
    "port_wts(wt_dict,'rnn',0,keras_model,'bidir_1.*/{}_lstm1/',reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting weigths for:bidir_2_1/forward_lstm2/kernel:0\n",
      "setting weigths for:bidir_2_1/forward_lstm2/bias:0\n",
      "setting weigths for:bidir_2_1/forward_lstm2/recurrent_kernel:0\n",
      "setting weigths for:bidir_2_1/backward_lstm2/kernel:0\n",
      "setting weigths for:bidir_2_1/backward_lstm2/bias:0\n",
      "setting weigths for:bidir_2_1/backward_lstm2/recurrent_kernel:0\n"
     ]
    }
   ],
   "source": [
    "#Porting Bi-LSTM layer - 2\n",
    "port_wts(wt_dict,'rnn',1,keras_model,'bidir_2.*/{}_lstm2/',reverse=False)\n",
    "port_wts(wt_dict,'rnn',1,keras_model,'bidir_2.*/{}_lstm2/',reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Saving ported model\n",
    "keras_model.save('Keras_CoVe.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
